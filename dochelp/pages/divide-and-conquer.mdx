---
title: A new approach to divide and conquer
description: "An empirical approach to k way merge sorting with visualization and tested data"
---

import ThumbNail from "../components/thumbnail";
import Image from "next/image";
import Implementation from "../components/links";

# Divide and Conquer

<ThumbNail />

## Abstract

Algorithmicians employ various approaches to simplify a problem and solve them efficiently, one such approach is Divide and Conquer, where the problem is broken down into smaller subproblems where each problem can be solved in a fast manner . Usually divide and conquer algorithms apply division at orders of 2 to achieve logarithmic complexity, in this paper, we discuss the effect of n order division. The implementation of the same has been attached as a reference.

## Introduction

Divide and conquer algorithm initially dates back to the historic era(s) of Babylonia and Euclid where a search tree was implemented using [BST](https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-fall-2011/resources/lecture-5-binary-search-trees-bst-sort/). In this particular context,
the case of the algorithm MergeSort is being discussed.

### Pseudocode

#### General Approach to merge-sort

```Pseudocode
- Given a problem of size n, divide into the problem into 2 smaller subproblems
- Keep recursively dividing till n is small enough
- At the smallest size, solve the problem and merge it with the smallest solution
```

#### Proposed Approach

```Pseudocode
- Given a problem of size n, divide into the problem into n smaller subproblems
- Keep recursively dividing till n is small enough
- At the smallest size, solve the problem and merge it with the smallest solution
```

### Visualization

Now, if one tries to find asymptotic time complexity, the time complexity O(n log n )
Going by the rules of recurrence relation, it can be easily understood that the logarithm term here takes log<sub>2</sub>x. This paper, tries to delve into the possibility of using algorithms that involve complexities log<sub>3</sub>n, log<sub>4</sub>n and the trade-offs associated there with.

<Image
  src="/division.png"
  height={500}
  width={800}
  alt="Implementation of a 4 way sort"
/>

## Methods

Just as any divide-conquer and combine method, this problem too has two different sub-problems,
the first part where the array is divided into sub-arrays of n sizes, and the second part where the array is merged.
The merging problem is consistent throughout the sub-divisions, i.e across the cases, the same merging algorithm will be implemented. This is done to reduce the operational complexity of the algorithm.

<br />
<em className="font-semibold">
  {" "}
  With increase in complexity, the number of O(n) iterations of the same will increase
  by a factor of n. This can massively increase after n greater than 4. It will take
  the sum of n numbers from 0 when we are dividing the given array into problems
  of size n.
</em>

One can evaluate this by using Sum of N numbers rule:
`Sum of n numbers = n(n+1)*0.5`

### Division Method

```Pseudocode
- Find the length of the array
- Implement floor division by n
- Considering that as an index, form sub-array of start_ele to index
- Arr[0..floor_division], Arr[floor_division+1..2*floor_division],
- Arr[2*floor_division+1..3*floor_division] and so on
- Call merge-sort and merge recursively
```

### Merging Method

```Pseudocode
- Start
- Determine mid ‚Üê (end - start)/2
- Generate two new sub-arrays, of size mid
    - L[0..mid] and R [0..mid]
- Copy the elements of the original array into the new sub-arrays
- Iterate through the array
- Compare and swap the elements of the sub-arrays
- Stop
```

### Possible enhancements and Alternative approaches

A segmented merging as mentioned ahead can be implemented for n = 3 (6 loops) iterating through the sub-arrays,
making it a significantly more efficient algorithm.

## Discussions

The metrics of this experiment are as follows:

| Size   | Division Base | Time Taken (in nanoseconds) |
| ------ | ------------- | --------------------------- |
| 1000   | 3             | 396363                      |
| 10000  | 3             | 7970020                     |
| 100000 | 3             | 36462811                    |
| 1000   | 4             | 530112                      |
| 10000  | 4             | 3607643                     |
| 100000 | 4             | 31403842                    |
| 1000   | 5             | 520345                      |
| 100000 | 5             | 5033564                     |
| 100000 | 5             | 24772391                    |

## Results and Conclusions

Through the objective evaluation of the data, one can conclude the increase in divisions reduces the time taken for arrays of large elements,
but the increase in the number of iterations of the same algorithm increases the time taken for smaller arrays. At the same time, it is also recorded
that at n = 3, the time taken is significantly less than the other cases, and the time taken is almost constant for n = 4 and n = 5. The time taken is less
for a smaller array, thus taking a larger division base for a smaller array is not ideal. Through further research, one can establish the peak value/bounding values
for which the division by large algorithm will start becoming more efficient than the division by 2 algorithm.

**It must be duly noted that the peak value will be a part of the graph that takes a Gaussian Distribution and through sampling, one can determine the
peak value.**

## Acknowledgements and References

- Merge Sort proposed by John Von Neumann in 1945, The art of computer programming by Donald Knuth, ISBN 0-201-89685-0.
- Merge Sort by Cormen, Et al. ISBN 978-0-262-03293-3.

## Links to Implementation

<Implementation/>